{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP自然语言处理\n",
    "## 基本术语\n",
    "### 1、分词\n",
    "* 词是最小的能够独立活动的有意义的语言成分，英文单词之间是以𥩂作为自然分界符的，而汉语是以基本的书写单位，词语之间没有明显的区分标记，因此，中文词语分析是中文中的与 都存在分词的需求，不过相较而言，单词本来就有空格进行分害虫，所以处理起来相对方便。但是由于中文是没有分隔符的，所以分词的问题就比较重要。\n",
    "### 2、词性标注\n",
    "* 基于机器学习的方法里，往往需要对词的词性进行标注。词性一般是指动词、名词、形容词等。标注的目的是表征词的一种隐藏状态，隐藏状态构成的转移就枹成了状态转移序列。\n",
    "### 3、命名实体识别（NER，Named Entity Recognition）\n",
    "* 命名实体识别是指从文本中识别具有特定类别的实体（通常是名词）例如人名，地名、机构名、专有名词等。\n",
    "### 4、句法分析（syntax parsing）\n",
    "* 句法分析往往是一种基于规则的专家系统。当然也不是说它不能用统计学的方法进行，不过最初的时候，还是利用语言宵专家的知识来的。分析 的目的是句子中各个成分的依赖关系。所以往往最终生成的 是一顶分析树。。句法分析可以解决传统词代模型不考虑上下文件的问题。\n",
    "### 5、指代消解（anaphora resolution）\n",
    "* 中文代词出现的频率很高，它的作用是用来表征前文出现过的人名、地名等。\n",
    "### 6、情感识别（emotion recognition）\\\n",
    "* 所谓情感识别，本质上是分类问题，经常被应用在舆论分析等领域。一般可以分为两类，即正面、负面，也可以是三类，在前面的基础上，再加上中性类别。一般来说，在电商企业，情感识别可以分析商品评价的好坏，以此作为下一个环节的证券依据。通过可以基于词袋模型+分类器，或者现在浒的词微量模型+RNN。经过测试发现，后者比前者准确率略有提升。\n",
    "### 7、纠错（correction）\n",
    "* 自动纠错在搜索技术以及输入法利用得很多。由于用户的输入出错的可能性比较大，出错的场景也比较多。所以，我们需要一个纠错系统。具体做法有很多，可以基于N-Gram进行纠错，也可以通过字典树，有限状态机等方法进行纠错。\n",
    "### 8、问题系统（QA system）\n",
    "* 这是一种类似机器人的人工智能系统。比较著名的有：苹果Siri、IBM Watson、微软小冰等。问答系统往往需要请问识别、合成，自然语言理解知识图谱等级多项技术的配合才会实现得比较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 知识结构\n",
    "* 句法语义分析：针对目标句子，进行各种句法分析，如分词、词性标记、命名实体识别及链接、句法分析、语义角色识别和多义词消歧等。\n",
    "* 关键词抽取：抽取目标文本中的主要信息，比如从一条新闻中抽取关键信息。主要是了解是谁，于何时、为何、对谁、做了何事、产生了有什么结果。涉及实体识别、时间抽取、因果关系等级多项关键技术。\n",
    "* 文本挖掘：主要包含了对文本的聚类、分粝、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的呈现界面。\n",
    "* 机器翻译：将输入的源语言文本通过自动翻译转化为另一种语言的文本。根据输入数据类型的不同，可为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则到二十年前的基于统计的方法 ，再到今天的基于深度学习（编解码）的方法\n",
    "，逐渐形成了一套比较严谨的方法体系。\n",
    "* 信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋以不同的来建立索引，也可以使用算法模型来建立更加深层的索引。。时，首先对输入进行分析，然后在索引里面查找匹配的候选文档，再根据一个机制把候选文档排序，最后输出排序得分最高的文档。\n",
    "* 问答系统：针对基本个自然语言表达的问题，由问答系统给出一个業的答案。需要对自然语言查询语句进行语义分析，钖褓、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个机制找出最佳的答案。\n",
    "* 对话系统：系统通过多回合对话，跟用户进行聊天、回答、完成某些任务。主要涉及用户意图理解、通过聊天引擎、对话管理等技术。此外，为上体现上下文相关，要具备多软对话能力。同时，为了体现个性化，对话系统还需要莀𥲠帮个性化回复。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语料库\n",
    "* 中文维基百科\n",
    "* 搜狗新闻语料库\n",
    "* IMDB情感分析语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "利用一个爬虫抓取到网络中的信息\n",
      "根据用户的需求，爬虫可以有主题爬虫和通过爬虫之分\n"
     ]
    }
   ],
   "source": [
    "### 正则表达式（爬虫）\n",
    "import re \n",
    "text_string = '文本是最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通过爬虫之分。'\n",
    "regex = '爬虫'\n",
    "p_string = text_string.split('。') #以句号为分隔符通过split切分\n",
    "for line in p_string:\n",
    "    if re.search(regex,line) is not None: #search方法是用来查找匹配当前行是否匹配这个regex,返回的是一个match对象\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 匹配任意字符\n",
    "* . 匹配任意一个字符\n",
    "* ^ 匹配开始的字符串\n",
    "* $ 匹配结尾的字符串\n",
    "* [] 匹配多个字符串 #\"[bcr]at\"代表的是匹配“bat”,\"cat\",\"rat\".i fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 抽取所有的年份\n",
    "import re\n",
    "years_string = '2016 was a good year,but 2017 will be better!'\n",
    "years = re.findall('[2][0-9]{3}',years_string)\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文分词简介\n",
    "* 自中文自动分词被提出以来，历经将近30年的探索，提出了很多方法，可主要归纳为“规则分词”“统计分词”和“混合分词”（规则+统计）这三个主要流派。规则分词是最早的方法，主要是通过人工设立词库，按照一定方式进行匹配切分，其实现简单高效，但对新词很难进行处理。随后统计机器学习技术的举，应用于分词任务上后，就有了统计分词，能够较好应对新发现等特殊场景。然而实践中，单纯的统计分词也有缺陷，那就是太过于依赖语料的质量，因此实践中多是这两种方法的结合，即混合分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba的三种分词模式\n",
    "* 精确模式： 试图将句子最精确的地切开，适合文本分析。\n",
    "* 全模式： 把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义。\n",
    "* 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式: 中文/分词/是/文本/文本处理/本处/处理/不可/不可或缺/或缺/的/一步//\n",
      "精确模式: 中文/分词/是/文本处理/不可或缺/的/一步/！\n",
      "默认精确模式: 中文/分词/是/文本处理/不可或缺/的/一步/！\n",
      "搜索引擎模式 中文/分词/是/文本/本处/处理/文本处理/不可/或缺/不可或缺/的/一步/！\n"
     ]
    }
   ],
   "source": [
    "### 下面是三种模式的对比\n",
    "import jieba\n",
    "sent = '中文分词是文本处理不可或缺的一步！'\n",
    "seg_list = jieba.cut(sent,cut_all=True)\n",
    "print('全模式:','/'.join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut(sent,cut_all=False)\n",
    "print('精确模式:','/'.join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut(sent)\n",
    "print('默认精确模式:','/'.join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(sent)\n",
    "print('搜索引擎模式','/'.join(seg_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
