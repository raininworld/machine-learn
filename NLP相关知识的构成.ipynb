{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP自然语言处理\n",
    "## 基本术语\n",
    "### 1、分词\n",
    "* 词是最小的能够独立活动的有意义的语言成分，英文单词之间是以𥩂作为自然分界符的，而汉语是以基本的书写单位，词语之间没有明显的区分标记，因此，中文词语分析是中文中的与 都存在分词的需求，不过相较而言，单词本来就有空格进行分害虫，所以处理起来相对方便。但是由于中文是没有分隔符的，所以分词的问题就比较重要。\n",
    "### 2、词性标注\n",
    "* 基于机器学习的方法里，往往需要对词的词性进行标注。词性一般是指动词、名词、形容词等。标注的目的是表征词的一种隐藏状态，隐藏状态构成的转移就枹成了状态转移序列。\n",
    "### 3、命名实体识别（NER，Named Entity Recognition）\n",
    "* 命名实体识别是指从文本中识别具有特定类别的实体（通常是名词）例如人名，地名、机构名、专有名词等。\n",
    "### 4、句法分析（syntax parsing）\n",
    "* 句法分析往往是一种基于规则的专家系统。当然也不是说它不能用统计学的方法进行，不过最初的时候，还是利用语言宵专家的知识来的。分析 的目的是句子中各个成分的依赖关系。所以往往最终生成的 是一顶分析树。。句法分析可以解决传统词代模型不考虑上下文件的问题。\n",
    "### 5、指代消解（anaphora resolution）\n",
    "* 中文代词出现的频率很高，它的作用是用来表征前文出现过的人名、地名等。\n",
    "### 6、情感识别（emotion recognition）\\\n",
    "* 所谓情感识别，本质上是分类问题，经常被应用在舆论分析等领域。一般可以分为两类，即正面、负面，也可以是三类，在前面的基础上，再加上中性类别。一般来说，在电商企业，情感识别可以分析商品评价的好坏，以此作为下一个环节的证券依据。通过可以基于词袋模型+分类器，或者现在浒的词微量模型+RNN。经过测试发现，后者比前者准确率略有提升。\n",
    "### 7、纠错（correction）\n",
    "* 自动纠错在搜索技术以及输入法利用得很多。由于用户的输入出错的可能性比较大，出错的场景也比较多。所以，我们需要一个纠错系统。具体做法有很多，可以基于N-Gram进行纠错，也可以通过字典树，有限状态机等方法进行纠错。\n",
    "### 8、问题系统（QA system）\n",
    "* 这是一种类似机器人的人工智能系统。比较著名的有：苹果Siri、IBM Watson、微软小冰等。问答系统往往需要请问识别、合成，自然语言理解知识图谱等级多项技术的配合才会实现得比较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 知识结构\n",
    "* 句法语义分析：针对目标句子，进行各种句法分析，如分词、词性标记、命名实体识别及链接、句法分析、语义角色识别和多义词消歧等。\n",
    "* 关键词抽取：抽取目标文本中的主要信息，比如从一条新闻中抽取关键信息。主要是了解是谁，于何时、为何、对谁、做了何事、产生了有什么结果。涉及实体识别、时间抽取、因果关系等级多项关键技术。\n",
    "* 文本挖掘：主要包含了对文本的聚类、分粝、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的呈现界面。\n",
    "* 机器翻译：将输入的源语言文本通过自动翻译转化为另一种语言的文本。根据输入数据类型的不同，可为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则到二十年前的基于统计的方法 ，再到今天的基于深度学习（编解码）的方法\n",
    "，逐渐形成了一套比较严谨的方法体系。\n",
    "* 信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋以不同的来建立索引，也可以使用算法模型来建立更加深层的索引。。时，首先对输入进行分析，然后在索引里面查找匹配的候选文档，再根据一个机制把候选文档排序，最后输出排序得分最高的文档。\n",
    "* 问答系统：针对基本个自然语言表达的问题，由问答系统给出一个業的答案。需要对自然语言查询语句进行语义分析，钖褓、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个机制找出最佳的答案。\n",
    "* 对话系统：系统通过多回合对话，跟用户进行聊天、回答、完成某些任务。主要涉及用户意图理解、通过聊天引擎、对话管理等技术。此外，为上体现上下文相关，要具备多软对话能力。同时，为了体现个性化，对话系统还需要莀𥲠帮个性化回复。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语料库\n",
    "* 中文维基百科\n",
    "* 搜狗新闻语料库\n",
    "* IMDB情感分析语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "利用一个爬虫抓取到网络中的信息\n",
      "根据用户的需求，爬虫可以有主题爬虫和通过爬虫之分\n"
     ]
    }
   ],
   "source": [
    "### 正则表达式（爬虫）\n",
    "import re \n",
    "text_string = '文本是最重要的来源无疑是网络。我们要把网络中的文本获取形成一个文本数据库。利用一个爬虫抓取到网络中的信息。爬取的策略有广度爬取和深度爬取。根据用户的需求，爬虫可以有主题爬虫和通过爬虫之分。'\n",
    "regex = '爬虫'\n",
    "p_string = text_string.split('。') #以句号为分隔符通过split切分\n",
    "for line in p_string:\n",
    "    if re.search(regex,line) is not None: #search方法是用来查找匹配当前行是否匹配这个regex,返回的是一个match对象\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 匹配任意字符\n",
    "* . 匹配任意一个字符\n",
    "* ^ 匹配开始的字符串\n",
    "* $ 匹配结尾的字符串\n",
    "* [] 匹配多个字符串 #\"[bcr]at\"代表的是匹配“bat”,\"cat\",\"rat\".i fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 抽取所有的年份\n",
    "import re\n",
    "years_string = '2016 was a good year,but 2017 will be better!'\n",
    "years = re.findall('[2][0-9]{3}',years_string)\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文分词简介\n",
    "* 自中文自动分词被提出以来，历经将近30年的探索，提出了很多方法，可主要归纳为“规则分词”“统计分词”和“混合分词”（规则+统计）这三个主要流派。规则分词是最早的方法，主要是通过人工设立词库，按照一定方式进行匹配切分，其实现简单高效，但对新词很难进行处理。随后统计机器学习技术的举，应用于分词任务上后，就有了统计分词，能够较好应对新发现等特殊场景。然而实践中，单纯的统计分词也有缺陷，那就是太过于依赖语料的质量，因此实践中多是这两种方法的结合，即混合分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba的三种分词模式\n",
    "* 精确模式： 试图将句子最精确的地切开，适合文本分析。\n",
    "* 全模式： 把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义。\n",
    "* 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式: 中文/分词/是/文本/文本处理/本处/处理/不可/不可或缺/或缺/的/一步//\n",
      "精确模式: 中文/分词/是/文本处理/不可或缺/的/一步/！\n",
      "默认精确模式: 中文/分词/是/文本处理/不可或缺/的/一步/！\n",
      "搜索引擎模式 中文/分词/是/文本/本处/处理/文本处理/不可/或缺/不可或缺/的/一步/！\n"
     ]
    }
   ],
   "source": [
    "### 下面是三种模式的对比\n",
    "import jieba\n",
    "sent = '中文分词是文本处理不可或缺的一步！'\n",
    "seg_list = jieba.cut(sent,cut_all=True)\n",
    "print('全模式:','/'.join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut(sent,cut_all=False)\n",
    "print('精确模式:','/'.join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut(sent)\n",
    "print('默认精确模式:','/'.join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(sent)\n",
    "print('搜索引擎模式','/'.join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词、词性标注，和命名实体识别。作为中文信息处理中基础性关键技术，它们是自然语言处理中在词层面的三姐妺，相互联系和影响。在合名实体识别中，当我们在切完词、完词性后，再做识别任务，效果要比单纯的字标注要好很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关系词提取算法\n",
    "* 对文本聚类、分类、自动摘要等起重要的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键词提取技术概述\n",
    "* 在信息爆炸的时代，很多信息我们无法全面接收，我们需要从中筛选出一些我们感的或者说对我们有用的信息进行接收，怎么选择呢，关键词就是其中一个很好的方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键词提取算法TF/IDF算法\n",
    "* TF-IDF算法（Term Frequency-Inverse Document Frequency，词频-逆文档频次算法）是一种基于统计的计算方法，常用于评估在一个文档集中一个对某份文档的重要程度。这种作用显然很符合关键词的需求，一个词对文档越重要，那就越可能是文档的关键词，常TF-IDF算法应用于关键词提取中。\n",
    "* 从算法的名称就可以看出，TF-IDF算法由两部分组成：TF算法以及IDF算法。TF算法是统计一个词在一篇文档中出现的频次，其基本思想是，一个词在文档中出现的次数越多，则其对文档的表达能力也就越强。则IDF算法则是统计一个词在文档集的多少个文档中出现，其基本的思想是如果一个词在越少的文档中出现，则其对文档的区分能力也就越强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank算法\n",
    "* TextRank算法则是可以脱离语料库的背景,仅对单篇文档进行分析就可以该文档的关键词.这也是TextRank算法的一个重要特点.TextRank算法最早用于文档的自动摘要,基于句子维度的分析,利用TextRank对每个句子进行打分,挑选出分数最高的n个句子作为文档的关键句,以达到自动摘要的效果."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank网页排名算法\n",
    "* 1)链接数量.一个网页被越多的其他网页链接,说明这个网页越重要.\n",
    "* 2)链接质量.一个网页被一个越高权值的网页链接,也能表明这个网页越重要."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练一个关键词算法需要以下几个步骤:\n",
    "\n",
    "* 1)加载已有的文档数据集.\n",
    "* 2)加载停用词表.\n",
    "* 3)对数据集中的文档进行分词.\n",
    "* 4)根据停用词表,过滤干扰词.\n",
    "* 5)根据数据集训练算法.\n",
    "* 而根据训练好的关键词提取算法对新文档进行关键词提取要经过以下几个环节:\n",
    "* 1)对新文档进行分词\n",
    "* 2)根据停用词表,过滤二扰词.\n",
    "* 3)根据训练好的算法提取关键词."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'他骑自选车云了菜市场'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 句法分析\n",
    "import jieba\n",
    "string = '他骑自选车云了菜市场'\n",
    "seg_list = jieba.cut(string,cut_all=False,HMM=True)\n",
    "seg_str = ''.join(seg_list)\n",
    "seg_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本向量化\n",
    "* 文本表示是自然语言处理中的基础工作,文本表示的好坏直接影响到整个自然语言处理系统的性能,因此,研究者们投入了大量的人力物力来研究文本表示方法,以期提高自然语言处理系统的性能.在自然语言处理研究领域,文本微量化是𤧵泊一种重要方式.顾名思义,文本向量化就是将文本表示成一系列能够表达文本语义的微量.无论是中文还是英文,词语都是表达文本处理的最单元.当前阶段,对文本向量化大部分的研究都是通过词向量化实现的.与此同时,也有相当一部分得出将文章或者句子作为文本处理的单元,于是产生了doc2vec和str2vec技术.\n",
    "* 近年来,随着互联网技术的发展,互联网上的数据急剧增加.大量无标注的数据产生,使得研究者将注意力转移到利用无标注数据挖掘有价值的信息上来,词向量技术就是为了利用神经网络从大量无标注的文本中提取有用信息而产生的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 情感分析技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习的训练要素\n",
    "* 机器学习的核心在于建模和算法，学习得到的参数只是一个结果。\n",
    "### 机器学习训练的要素\n",
    "* 成功训练一个模型需要四个要素：数据，转换数据的模型，衡量模型好坏的损失函数和一个调整模型权重以便最小化损失函数的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本分类大致分为以下几个步骤\n",
    "* 1）定义阶段：定义数据以及分类体系，具体分为哪些类别，需要哪些数据。\n",
    "* 2）数据预处理：对文档做分词、去停用词等准备工作。\n",
    "* 3）数据提取特征：对文档矩阵进行降维，提取训练集中最有用的特征。\n",
    "* 4）模型训练阶段：选择具体的分类模型以及算法，训练出文本分类器。\n",
    "* 5）评测阶段：在测试集上测试并评价分类器的性能。\n",
    "* 6）应用阶段：应用性能最高的分类模型对待分类文档进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最优化算法\n",
    "* 机器学习完成一个训练任务有三个要素：算法模型、目标函数、优化算法。优化机器学习问题的求解，本质上都是优化问题。最常见的求解方式就是迭代优化，也就是一次次不停地优化，俗称“打铁”，而这个过程持续的时间长短（迭代的次数），每次捶下去的力道（参数搜索的步长），火的冷热程度（参数更新的权重）等因素，都是靠优化算法来调节。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
